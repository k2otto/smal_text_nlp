

import matplotlib.pyplot as plt
import pandas as pd
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
import warnings
import gensim
from gensim.models import Word2Vec
import gzip
import json
import os
import re
import csv
import math
import numpy as np
from sklearn.experimental import enable_hist_gradient_boosting  # noqa
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.inspection import plot_partial_dependence
import numpy as np
from numpy.random import seed
from numpy.random import rand
import random
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
pd.set_option('display.max_columns', None)


path='C:/Users/User/Downloads'

os.chdir(path)


df= pd.read_csv('C:/Users/User/Downloads/Zaehler-Sample-Data_v_2.csv', sep=";",  dtype = str)

df.purpose= df['purpose'].str.lower()
df.zaehlernummer= df['zaehlernummer'].str.lower()


df = df.loc[~df.zaehlernummer.isna()]

candidate = [None] * len(df.purpose)

input_str=df.purpose
chars_to_remove = ['.', '!', '?',":",";","'","[","]","(",")", " "]
rx_to_be_removed = '[' + re.escape(''.join(chars_to_remove)) + ']'

candidate=input_str.str.findall("(?<=vst).*?\w+|(?<=zaehlernummer).*?\w+ ")
indicator_word1=candidate*0
indicator_word2=candidate*0
indicator_word3=candidate*0

correct_candidate=candidate*0

candidate1=candidate

candidate2=input_str.str.findall("(?<=zaehler-nr).*?\w+")
candidate3=input_str.str.findall("abschlag\s*(\S+)\s*(\S+)\s*")




for index, row in candidate.iteritems():
    candidate1.at[index] = re.sub(rx_to_be_removed, '', str(list(set(candidate1[index]))))


for index, row in candidate.iteritems():
    candidate2.at[index] = re.sub(rx_to_be_removed, '', str(list(set(candidate2[index]))))

for index, row in candidate.iteritems():
    if len(list(set(candidate3[index])))>0:
        candidate3.at[index] = re.sub(rx_to_be_removed, '', str(list(set(candidate3[index]))[-1][-1]))



for index, row in indicator_word1.iteritems():
    indicator_word1[index]=list(set(input_str.str.findall("\w*..(?=\s+"+str(df.zaehlernummer[index])+")")[index]))
    indicator_word2[index]=list(set(input_str.str.findall("(\S+)\s*"+str(df.zaehlernummer[index]))[index]))
    indicator_word3[index]=indicator_word2[index]


    indicator_word1.at[index] = re.sub(rx_to_be_removed, '', str(list(set(indicator_word1[index]))))
    indicator_word2.at[index] = re.sub(rx_to_be_removed, '', str(list(set(indicator_word2[index]))))
    indicator_word3.at[index] = re.sub(r'\s*\d+\s*', '', indicator_word2[index])

x=candidate1==df.zaehlernummer


print(x.mean())

#0.4528301886792453
print(indicator_word1.value_counts())
print(indicator_word2.value_counts())
print(indicator_word3.value_counts())



for index,row in candidate.iteritems():
    if candidate1[index]==df.zaehlernummer[index]:
        correct_candidate[index]=1
    elif candidate2[index]==df.zaehlernummer[index]:
        correct_candidate[index]=2
    elif candidate3[index]==df.zaehlernummer[index]:
         correct_candidate[index]=3


for index,row in correct_candidate.iteritems():
    if not correct_candidate[index]:
        correct_candidate.at[index]=9999

print(correct_candidate.value_counts())
##############################
# Regression

y=correct_candidate
y=y.to_numpy(dtype=object)
y=y.astype('int')




#X = np.c_[pd.get_dummies(indicator_word1, prefix='indicator_word1')]
X = pd.DataFrame(pd.get_dummies(indicator_word1, prefix='indicator_word1',dummy_na=True))
Z = pd.DataFrame(pd.get_dummies(indicator_word2, prefix='indicator_word2',dummy_na=True))
Y = pd.DataFrame(pd.get_dummies(indicator_word2, prefix='indicator_word3',dummy_na=True))

X = X.join(Z)



#X, y = make_classification(n_features=4, random_state=0)
clf = ExtraTreesClassifier(n_estimators=100, random_state=0)

#####################
# Split train test
train_fraction=0.66
train_length=round(len(y)*train_fraction)

train_index=random.sample(list(range(0, (len(y)))), train_length)
test_index=list(range(0, (len(y))))
test_index=list(np.setdiff1d(test_index,train_index))


clf.fit(X.iloc[train_index],y[train_index])

forecasts=clf.predict(X.iloc[test_index])

x=forecasts==y[test_index]
p=clf.predict_proba(X.iloc[test_index])

index_forecaste= [None] * len(y[test_index])

prediction_correct_value= [None] * len(y[test_index])
set_list=list(set(y))


for j in range(len(index_forecaste)):
    index_forecaste[j]=set_list.index(forecasts[j])
    prediction_correct_value[j]=p[j,index_forecaste[j]]




print(p)
print("###########")
print("Mean accuracy")
print(x.mean())
print("Lowest accuracy rate")
print(min(prediction_correct_value))


importances = clf.feature_importances_
std = np.std([tree.feature_importances_ for tree in clf.estimators_],
             axis=0)
indices = np.argsort(importances)[::-1]

# Print the feature ranking

(pd.Series(clf.feature_importances_, index=X.columns)
   .nlargest(15)
   .plot(kind='barh'))





#class zaehler_extract:
#    def __init__(self,input,name):
#        self.name=name
#        self.input=input

#    def

#df.text= df['text'].str.lower()

#print(df.text.head(10))

#df2=df


   ###